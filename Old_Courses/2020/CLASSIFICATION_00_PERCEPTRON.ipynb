{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# reorder columns - pass a list as a list and index\n#order we want\ncols = ['Student', 'Class','Gender', 'Math', 'Science', 'English','History']\n# overwrite the old dataframe with the same dataframe but new column order\ngrades = grades[cols]"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## PERCEPTRON CLASSIFICATION"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn import datasets\nimport numpy as np",
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "iris = datasets.load_iris()\nX = iris.data[:, [2, 3]]\ny = iris.target\nprint('Class labels:', np.unique(y))\n# ",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Class labels: [0 1 2]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Using the train_test_split function from scikit-learn's model_selection module, we randomly split the X and y arrays into 30 percent test data (45 examples) and 70 percent training data (105 examples).\n\nNote that the \"train_test_split\" function already shuffles the training datasets internally before splitting; otherwise, all examples from class 0 and class 1 would have ended up in the training datasets, and the test dataset would consist of 45 examples from class 2. Via the random_state parameter, we provided a fixed random seed (\"random_state=1\") for the internal pseudo-random number generator that is used for shuffling the datasets prior to splitting. Using such a fixed \"random_state\" ensures that our results are reproducible. Lastly, we took advantage of the built-in support for stratification via \"stratify=y\". In this context, stratification means that the train_test_split method returns training and test subsets that have the same proportions of class labels as the input dataset(same amount of classifications (0,1 or 2) per segment(train or test)). "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1, stratify=y)\n\nsc = StandardScaler() #instance of StandardScaler object\nsc.fit(X_train) # Using the fit method, StandardScaler estimated the parameters, ùúá (sample mean) and ùúé (standard deviation), \n                # for each feature dimension from the \"training data\". \n    \nX_train_std = sc.transform(X_train) # once StandardScaler fitted, we proceed to apply/transform our training data \n                                    # and stored it on new variable.\nX_test_std = sc.transform(X_test)   # and same process applied to \"test data\"",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Having standardized the training data, we can now train a perceptron model. Most algorithms in scikit-learn already support multiclass classification by default via the one-vs.-rest (OvR) method, which allows us to feed the three flower classes to the\nperceptron all at once."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.linear_model import Perceptron\n\nppn = Perceptron(eta0=0.1, random_state=1) # Here, the model parameter, eta0, is equivalent to the learning rate, eta, that we \n                                           # used in our own perceptron implementation, and the n_iter parameter defines the \n                                           # number of epochs (passes over the training dataset).\n                                           # \"random_state\" for seed purposes and \"eta0\" is based on trial an error(so far...).\n        \nppn.fit(X_train_std, y_train)            # after obj initialization we proceed to fit the model with data from previous steps",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/home/nbuser/anaconda3_501/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in Perceptron in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n  FutureWarning)\n",
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "execution_count": 4,
          "data": {
            "text/plain": "Perceptron(alpha=0.0001, class_weight=None, early_stopping=False, eta0=0.1,\n      fit_intercept=True, max_iter=None, n_iter=None, n_iter_no_change=5,\n      n_jobs=None, penalty=None, random_state=1, shuffle=True, tol=None,\n      validation_fraction=0.1, verbose=0, warm_start=False)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": " y_pred = ppn.predict(X_test_std)   # we can make predictions via the predict method\n\nprint('Misclassified examples: %d' % (y_test != y_pred).sum()) # comparing real values on \"y_test\" vs our result from previous line\n\n\n# Instead of the misclassification error, many machine learning practitioners report the classification accuracy of a model\n# scikit-learn implements a large variety of different performance metrics that are available via the metrics module including \n# accuracy of a model\n\nfrom sklearn.metrics import accuracy_score\nprint('Scikit-learn metric Accuracy: %.3f' % accuracy_score(y_test, y_pred))\n\n# Alternatively, each classifier in scikit-learn has a score method, which computes a classifier's prediction accuracy \n# by combining the predict call with accuracy_score\nprint('Scikit-learn Score Method: %.3f' % ppn.score(X_test_std, y_test))",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Misclassified examples: 11\nScikit-learn metric Accuracy: 0.756\nScikit-learn Score Method: 0.756\n",
          "name": "stdout"
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}